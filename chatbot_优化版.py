import streamlit as st
import os
import hashlib
import time
from huggingface_hub import InferenceClient
import asyncio
from concurrent.futures import ThreadPoolExecutor
import requests
import json

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Chatbot (ä¼˜åŒ–ç‰ˆ)",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ============================================================================
# ä¼˜åŒ–1: åˆå§‹åŒ–æ‰€æœ‰ä¼šè¯çŠ¶æ€ï¼ˆåŒ…æ‹¬ç¼“å­˜å’Œå®¢æˆ·ç«¯ï¼‰
# ============================================================================

if "messages" not in st.session_state:
    st.session_state.messages = []

# ç¼“å­˜ï¼šå­˜å‚¨ç›¸åŒé—®é¢˜çš„å›ç­”ï¼Œé¿å…é‡å¤è°ƒç”¨ API
if "cache" not in st.session_state:
    st.session_state.cache = {}

# å®¢æˆ·ç«¯ï¼šå¤ç”¨ InferenceClientï¼Œé¿å…é‡å¤åˆ›å»º
if "client" not in st.session_state:
    st.session_state.client = None

# å®¢æˆ·ç«¯é…ç½®ï¼šç”¨äºæ£€æµ‹ Token æˆ–æ¨¡å‹æ˜¯å¦æ”¹å˜
if "client_config" not in st.session_state:
    st.session_state.client_config = {"token": None, "model": None}

# ä¸Šä¸€æ¬¡é€‰æ‹©çš„æ¨¡å‹ï¼šç”¨äºæ£€æµ‹æ¨¡å‹åˆ‡æ¢
if "previous_model" not in st.session_state:
    st.session_state.previous_model = None

# é€Ÿç‡é™åˆ¶ï¼šè®°å½•æœ€åä¸€æ¬¡è¯·æ±‚æ—¶é—´
if "last_request_time" not in st.session_state:
    st.session_state.last_request_time = 0

# è¯·æ±‚è®¡æ•°ï¼šç»Ÿè®¡è¯·æ±‚æ¬¡æ•°ï¼ˆå¯é€‰ï¼Œç”¨äºç›‘æ§ï¼‰
if "request_count" not in st.session_state:
    st.session_state.request_count = 0

# æ¶ˆæ¯å†å²æœ€å¤§é•¿åº¦
MAX_MESSAGE_HISTORY = 20  # æœ€å¤šä¿ç•™ 20 æ¡æ¶ˆæ¯
RATE_LIMIT_SECONDS = 2    # é€Ÿç‡é™åˆ¶ï¼šæ¯ 2 ç§’æœ€å¤šä¸€æ¬¡è¯·æ±‚

# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šè·å–æˆ–åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä¼˜åŒ–1: å®¢æˆ·ç«¯å¤ç”¨ï¼‰
# ============================================================================

def get_client(token):
    """
    è·å–æˆ–åˆ›å»º InferenceClient å®¢æˆ·ç«¯
    
    ä¼˜åŒ–ç‚¹ï¼š
    - å¦‚æœå®¢æˆ·ç«¯å·²å­˜åœ¨ä¸” Token æœªæ”¹å˜ï¼Œç›´æ¥å¤ç”¨
    - å¦‚æœ Token æ”¹å˜ï¼Œåˆ›å»ºæ–°å®¢æˆ·ç«¯
    - é¿å…æ¯æ¬¡è¯·æ±‚éƒ½åˆ›å»ºæ–°å®¢æˆ·ç«¯ï¼ŒèŠ‚çœèµ„æº
    """
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°å®¢æˆ·ç«¯
    if (st.session_state.client is None or 
        st.session_state.client_config["token"] != token):
        # Token æ”¹å˜æˆ–å®¢æˆ·ç«¯ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°å®¢æˆ·ç«¯
        st.session_state.client = InferenceClient(token=token if token else None)
        st.session_state.client_config["token"] = token
        st.session_state.client_config["model"] = None  # æ¨¡å‹æ”¹å˜ä¸å½±å“å®¢æˆ·ç«¯
    
    return st.session_state.client

# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆç¼“å­˜é”®ï¼ˆä¼˜åŒ–2: æ·»åŠ ç¼“å­˜ï¼‰
# ============================================================================

def get_cache_key(prompt, model_name, recent_messages):
    """
    ç”Ÿæˆç¼“å­˜é”®
    
    å‚æ•°ï¼š
    - prompt: ç”¨æˆ·å½“å‰è¾“å…¥
    - model_name: æ¨¡å‹åç§°
    - recent_messages: æœ€è¿‘çš„å¯¹è¯å†å²ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰
    
    è¿”å›ï¼šMD5 å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®
    """
    # å°†æç¤ºè¯ã€æ¨¡å‹åç§°å’Œæœ€è¿‘æ¶ˆæ¯ç»„åˆæˆå­—ç¬¦ä¸²
    cache_string = f"{model_name}:{prompt}:{str(recent_messages)}"
    # ç”Ÿæˆ MD5 å“ˆå¸Œå€¼
    return hashlib.md5(cache_string.encode()).hexdigest()

# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥ç¼“å­˜ï¼ˆä¼˜åŒ–2: æ·»åŠ ç¼“å­˜ï¼‰
# ============================================================================

def get_cached_reply(cache_key):
    """
    ä»ç¼“å­˜ä¸­è·å–å›å¤
    
    è¿”å›ï¼šç¼“å­˜çš„å›å¤ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
    """
    return st.session_state.cache.get(cache_key)

def save_to_cache(cache_key, reply):
    """
    å°†å›å¤ä¿å­˜åˆ°ç¼“å­˜
    
    ä¼˜åŒ–ï¼šé™åˆ¶ç¼“å­˜å¤§å°ï¼Œé˜²æ­¢å†…å­˜å ç”¨è¿‡å¤§
    """
    # å¦‚æœç¼“å­˜å¤ªå¤§ï¼Œæ¸…é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆç®€å•ç­–ç•¥ï¼šä¿ç•™æœ€è¿‘ 50 æ¡ï¼‰
    if len(st.session_state.cache) > 50:
        # åˆ é™¤æœ€æ—§çš„ä¸€æ¡ï¼ˆå­—å…¸çš„é”®æ˜¯æ— åºçš„ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        oldest_key = next(iter(st.session_state.cache))
        del st.session_state.cache[oldest_key]
    
    st.session_state.cache[cache_key] = reply

# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šé™åˆ¶æ¶ˆæ¯å†å²ï¼ˆä¼˜åŒ–3: é™åˆ¶æ¶ˆæ¯å†å²ï¼‰
# ============================================================================

def limit_message_history():
    """
    é™åˆ¶æ¶ˆæ¯å†å²é•¿åº¦ï¼Œé˜²æ­¢æ— é™å¢é•¿
    
    ä¼˜åŒ–ç‚¹ï¼š
    - åªä¿ç•™æœ€è¿‘çš„ N æ¡æ¶ˆæ¯
    - é˜²æ­¢å†…å­˜å ç”¨è¿‡å¤§
    - é˜²æ­¢ API è°ƒç”¨æ—¶ token è¿‡å¤š
    """
    if len(st.session_state.messages) > MAX_MESSAGE_HISTORY:
        # ä¿ç•™æœ€è¿‘çš„ MAX_MESSAGE_HISTORY æ¡æ¶ˆæ¯
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGE_HISTORY:]

# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šé€Ÿç‡é™åˆ¶ï¼ˆé˜²æ­¢ API æ»¥ç”¨ï¼‰
# ============================================================================

def check_rate_limit():
    """
    æ£€æŸ¥é€Ÿç‡é™åˆ¶
    
    è¿”å›ï¼šTrue è¡¨ç¤ºå¯ä»¥ç»§ç»­ï¼ŒFalse è¡¨ç¤ºéœ€è¦ç­‰å¾…
    """
    current_time = time.time()
    time_since_last = current_time - st.session_state.last_request_time
    
    if time_since_last < RATE_LIMIT_SECONDS:
        return False, RATE_LIMIT_SECONDS - time_since_last
    
    st.session_state.last_request_time = current_time
    return True, 0

# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šå¤‡ç”¨ API è°ƒç”¨æ–¹æ³•ï¼ˆä½¿ç”¨ requests ç›´æ¥è°ƒç”¨ï¼‰
# ============================================================================

def call_api_direct(token, model_name, messages_for_api=None, prompt_text=None):
    """
    ç›´æ¥ä½¿ç”¨ requests è°ƒç”¨ Hugging Face APIï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
    
    è¿™ä¸ªæ–¹æ³•å¯ä»¥é¿å… InferenceClient çš„å“åº”è§£æé—®é¢˜
    æ”¯æŒæ–‡æœ¬ç”Ÿæˆå’Œå¯¹è¯æ¨¡å‹
    """
    # åˆ¤æ–­æ˜¯å¯¹è¯æ¨¡å‹è¿˜æ˜¯æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
    chat_models = [
        "moonshotai/Kimi-K2-Thinking",
        "deepseek-ai/DeepSeek-V3.2",
        "meta-llama/Llama-3.1-8B-Instruct"
    ]
    is_chat_model = model_name in chat_models or "chat" in model_name.lower() or "llama" in model_name.lower() or "kimi" in model_name.lower() or "deepseek" in model_name.lower() or "instruct" in model_name.lower()
    
    # ä½¿ç”¨æ–°çš„ API ç«¯ç‚¹ï¼ˆrouter.huggingface.coï¼‰
    # æ—§çš„ api-inference.huggingface.co å·²ä¸å†æ”¯æŒ
    
    if is_chat_model and messages_for_api:
        # å¯¹è¯æ¨¡å‹ï¼šå°è¯•ä½¿ç”¨æ–°çš„ router API
        # æ–° API å¯èƒ½éœ€è¦ä¸åŒçš„æ ¼å¼
        url = f"https://router.huggingface.co/models/{model_name}"
        headers = {
            "Authorization": f"Bearer {token}" if token else None,
            "Content-Type": "application/json"
        }
        
        # å°è¯• OpenAI å…¼å®¹æ ¼å¼
        payload = {
            "model": model_name,
            "messages": messages_for_api,
            "max_tokens": 150,
            "temperature": 0.7
        }
    else:
        # æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨ text generation API
        if not prompt_text:
            # å¦‚æœæ²¡æœ‰æä¾› prompt_textï¼Œä» messages æ„å»º
            if messages_for_api:
                prompt_text = "\n".join([
                    f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
                    for msg in messages_for_api[-5:]
                ])
                prompt_text += "\nAssistant:"
            else:
                raise ValueError("éœ€è¦æä¾› prompt_text æˆ– messages_for_api")
        
        # ä½¿ç”¨æ–°çš„ router API
        url = f"https://router.huggingface.co/models/{model_name}"
        headers = {
            "Authorization": f"Bearer {token}" if token else None,
            "Content-Type": "application/json"
        }
        
        # å°è¯•æ–°çš„ API æ ¼å¼
        payload = {
            "inputs": prompt_text,
            "parameters": {
                "max_new_tokens": 150,
                "temperature": 0.7,
                "return_full_text": False
            }
        }
    
    # ç§»é™¤ None å€¼
    headers = {k: v for k, v in headers.items() if v is not None}
    
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=60)
        
        if response.status_code == 200:
            result = response.json()
            
            # å¤„ç†ä¸åŒçš„å“åº”æ ¼å¼
            if isinstance(result, list) and len(result) > 0:
                if "generated_text" in result[0]:
                    generated = result[0]["generated_text"]
                    # å¦‚æœæ˜¯å¯¹è¯æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦æå–å›å¤éƒ¨åˆ†
                    if is_chat_model and "Assistant:" in generated:
                        return generated.split("Assistant:")[-1].strip()
                    return generated
                elif "text" in result[0]:
                    return result[0]["text"]
                elif "message" in result[0]:
                    return result[0]["message"].get("content", str(result[0]))
            elif isinstance(result, dict):
                if "generated_text" in result:
                    generated = result["generated_text"]
                    if is_chat_model and "Assistant:" in generated:
                        return generated.split("Assistant:")[-1].strip()
                    return generated
                elif "text" in result:
                    return result["text"]
                elif "choices" in result and len(result["choices"]) > 0:
                    # OpenAI æ ¼å¼çš„å“åº”
                    if "message" in result["choices"][0]:
                        return result["choices"][0]["message"].get("content", "")
                    return str(result["choices"][0])
            
            # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œè¿”å›å­—ç¬¦ä¸²è¡¨ç¤º
            return str(result)
        elif response.status_code == 410:
            # API ç«¯ç‚¹å·²åºŸå¼ƒ
            error_msg = "API ç«¯ç‚¹å·²æ›´æ–°ï¼Œæ­£åœ¨å°è¯•ä½¿ç”¨ InferenceClient..."
            raise Exception(f"API ç«¯ç‚¹å·²åºŸå¼ƒ: {error_msg}")
        elif response.status_code == 503:
            error_msg = response.json().get("error", "æ¨¡å‹æ­£åœ¨åŠ è½½")
            raise Exception(f"æ¨¡å‹æ­£åœ¨åŠ è½½ï¼Œè¯·ç¨åé‡è¯•: {error_msg}")
        else:
            error_text = response.text[:500] if response.text else "æœªçŸ¥é”™è¯¯"
            raise Exception(f"API è°ƒç”¨å¤±è´¥ ({response.status_code}): {error_text}")
    except requests.exceptions.Timeout:
        raise Exception("è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
    except requests.exceptions.ConnectionError:
        raise Exception("ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ")
    except Exception as e:
        raise e

# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šåŒæ­¥ API è°ƒç”¨ï¼ˆç”¨äºå¼‚æ­¥åŒ…è£…ï¼‰
# ============================================================================

def call_api_sync(client, model_name, messages_for_api, prompt_text):
    """
    åŒæ­¥è°ƒç”¨ APIï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼‰
    
    è¿™ä¸ªå‡½æ•°ä¼šåœ¨ ThreadPoolExecutor ä¸­æ‰§è¡Œï¼Œä¸ä¼šé˜»å¡ä¸»çº¿ç¨‹
    """
    try:
        # å¯¹è¯æ¨¡å‹åˆ—è¡¨
        chat_models_list = [
            "moonshotai/Kimi-K2-Thinking",
            "deepseek-ai/DeepSeek-V3.2",
            "meta-llama/Llama-3.1-8B-Instruct"
        ]
        
        if model_name in chat_models_list:
            # å¯¹è¯æ¨¡å‹
            response = client.chat_completion(
                model=model_name,
                messages=messages_for_api,
                max_tokens=150
            )
            # å®‰å…¨åœ°è®¿é—®å“åº”å†…å®¹
            if hasattr(response, 'choices') and len(response.choices) > 0:
                if hasattr(response.choices[0], 'message'):
                    return response.choices[0].message.content
                else:
                    return str(response.choices[0])
            else:
                return str(response)
        else:
            # æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
            response = client.text_generation(
                prompt_text,
                model=model_name,
                max_new_tokens=150,
                temperature=0.7,
                return_full_text=False  # åªè¿”å›æ–°ç”Ÿæˆçš„æ–‡æœ¬
            )
            # text_generation å¯èƒ½è¿”å›å­—ç¬¦ä¸²æˆ–ç”Ÿæˆå™¨
            if isinstance(response, str):
                return response
            elif hasattr(response, '__iter__'):
                # å¦‚æœæ˜¯ç”Ÿæˆå™¨æˆ–è¿­ä»£å™¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                try:
                    return ''.join(response) if not isinstance(response, str) else response
                except StopIteration:
                    # å¤„ç† StopIterationï¼ˆè¿­ä»£å™¨è€—å°½ï¼‰
                    return str(response) if response else "ç”Ÿæˆå¤±è´¥ï¼šå“åº”ä¸ºç©º"
            else:
                return str(response)
    except StopIteration as e:
        # ä¸“é—¨å¤„ç† StopIteration é”™è¯¯
        raise Exception(f"API å“åº”è§£æé”™è¯¯ï¼ˆStopIterationï¼‰: å¯èƒ½æ˜¯å“åº”æ ¼å¼ä¸ç¬¦åˆé¢„æœŸã€‚åŸå§‹é”™è¯¯: {str(e)}")
    except Exception as e:
        raise e

# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šå¼‚æ­¥ API è°ƒç”¨ï¼ˆä¼˜åŒ–4: å¼‚æ­¥è°ƒç”¨ï¼‰
# ============================================================================

def call_api_async(client, model_name, messages_for_api, prompt_text):
    """
    å¼‚æ­¥è°ƒç”¨ API
    
    ä¼˜åŒ–ç‚¹ï¼š
    - ä½¿ç”¨ ThreadPoolExecutor åœ¨åå°çº¿ç¨‹æ‰§è¡ŒåŒæ­¥ API è°ƒç”¨
    - ä¸é˜»å¡ Streamlit ä¸»çº¿ç¨‹
    - ç”¨æˆ·ç•Œé¢ä¿æŒå“åº”
    
    æ³¨æ„ï¼šStreamlit æœ¬èº«ä¸æ”¯æŒçœŸæ­£çš„å¼‚æ­¥ï¼Œè¿™é‡Œä½¿ç”¨çº¿ç¨‹æ± æ¨¡æ‹Ÿå¼‚æ­¥
    """
    # åˆ›å»ºçº¿ç¨‹æ± æ‰§è¡Œå™¨
    executor = ThreadPoolExecutor(max_workers=1)
    
    # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
    future = executor.submit(
        call_api_sync,
        client,
        model_name,
        messages_for_api,
        prompt_text
    )
    
    # ç­‰å¾…ç»“æœï¼ˆè¿™é‡Œä»ç„¶ä¼šé˜»å¡ï¼Œä½†åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ st.empty() å’Œè½®è¯¢æ¥æ˜¾ç¤ºè¿›åº¦
    try:
        result = future.result(timeout=60)  # 60 ç§’è¶…æ—¶
        return result
    except Exception as e:
        raise e
    finally:
        executor.shutdown(wait=False)

# ============================================================================
# ä¸»ç•Œé¢ï¼šä¾§è¾¹æ é…ç½®
# ============================================================================

with st.sidebar:
    st.title("âš™ï¸ é…ç½®")
    
    # Token è¾“å…¥
    hf_token = st.text_input(
        "Hugging Face Token",
        type="password",
        value=os.environ.get("HF_TOKEN", ""),
        help="è¾“å…¥ä½ çš„ Hugging Face Tokenï¼ˆä»¥ hf_ å¼€å¤´ï¼‰"
    )
    
    # Token çŠ¶æ€æŒ‡ç¤º
    if hf_token:
        os.environ["HF_TOKEN"] = hf_token
        if hf_token.startswith("hf_"):
            st.success("âœ… Token æ ¼å¼æ­£ç¡®")
        else:
            st.error("âŒ Token æ ¼å¼é”™è¯¯ï¼ˆåº”ä»¥ hf_ å¼€å¤´ï¼‰")
    else:
        st.warning("âš ï¸ æœªè®¾ç½® Token")
        with st.expander("ğŸ“– å¦‚ä½•è·å– Token"):
            st.write("1. è®¿é—® https://huggingface.co/settings/tokens")
            st.write("2. ç™»å½•ä½ çš„ Hugging Face è´¦å·")
            st.write("3. ç‚¹å‡» 'New token' åˆ›å»ºæ–° Token")
            st.write("4. é€‰æ‹© 'Read' æƒé™ï¼ˆå…è´¹æ¨¡å‹åªéœ€è¦è¯»æƒé™ï¼‰")
            st.write("5. å¤åˆ¶ç”Ÿæˆçš„ Tokenï¼ˆä»¥ `hf_` å¼€å¤´ï¼‰")
            st.write("6. ç²˜è´´åˆ°ä¸Šé¢çš„è¾“å…¥æ¡†")
    
    # æ¨¡å‹é€‰æ‹©
    model_name = st.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        [
            "moonshotai/Kimi-K2-Thinking",  # Moonshot AI çš„ Kimi æ¨¡å‹ï¼ˆæ¨èï¼Œä¸­æ–‡å‹å¥½ï¼‰
            "deepseek-ai/DeepSeek-V3.2",  # DeepSeek V3.2 æ¨¡å‹ï¼ˆæ¨èï¼Œæ€§èƒ½å¼ºï¼‰
            "meta-llama/Llama-3.1-8B-Instruct",  # Llama 3.1ï¼ˆ8Bï¼Œçƒ­é—¨ï¼‰
        ],
        index=0,
        help="é€‰æ‹©è¦ä½¿ç”¨çš„ AI æ¨¡å‹ã€‚æ¨èä½¿ç”¨ Kimi æˆ– DeepSeekï¼ˆä¸­æ–‡å‹å¥½ï¼‰\næ³¨æ„ï¼šåˆ‡æ¢æ¨¡å‹ä¼šè‡ªåŠ¨æ¸…é™¤å¯¹è¯å†å²"
    )
    
    # æ£€æµ‹æ¨¡å‹åˆ‡æ¢ï¼Œè‡ªåŠ¨æ¸…é™¤å¯¹è¯å†å²
    if st.session_state.previous_model is not None and st.session_state.previous_model != model_name:
        # æ¨¡å‹å·²åˆ‡æ¢ï¼Œæ¸…é™¤å¯¹è¯å†å²
        st.session_state.messages = []
        st.session_state.cache = {}  # åŒæ—¶æ¸…é™¤ç¼“å­˜ï¼Œå› ä¸ºç¼“å­˜å¯èƒ½åŒ…å«æ—§æ¨¡å‹çš„å›å¤
        st.info(f"ğŸ”„ å·²åˆ‡æ¢åˆ° {model_name.split('/')[-1]}ï¼Œå¯¹è¯å†å²å·²æ¸…é™¤")
    
    # æ›´æ–°ä¸Šä¸€æ¬¡çš„æ¨¡å‹
    st.session_state.previous_model = model_name
    
    # é«˜çº§è®¾ç½®
    with st.expander("âš™ï¸ é«˜çº§è®¾ç½®"):
        use_cache = st.checkbox("å¯ç”¨ç¼“å­˜", value=True, help="ç¼“å­˜ç›¸åŒé—®é¢˜çš„å›ç­”ï¼Œæé«˜å“åº”é€Ÿåº¦")
        # é»˜è®¤ä½¿ç”¨ InferenceClientï¼ˆå·²æ›´æ–°æ”¯æŒæ–°ç«¯ç‚¹ï¼‰ï¼Œå¤‡ç”¨æ–¹æ³•ä½œä¸ºå¤‡é€‰
        use_direct_api = st.checkbox("ä½¿ç”¨å¤‡ç”¨ API æ–¹æ³•", value=False, 
                                    help="å¦‚æœé‡åˆ° API é”™è¯¯ï¼Œå¯ä»¥å°è¯•å¯ç”¨æ­¤é€‰é¡¹ï¼ˆé»˜è®¤ä½¿ç”¨ InferenceClientï¼‰")
        use_async = st.checkbox("å¼‚æ­¥è°ƒç”¨", value=False, help="ä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰")
        max_history = st.slider("æœ€å¤§æ¶ˆæ¯å†å²", min_value=5, max_value=50, value=MAX_MESSAGE_HISTORY, 
                                help="é™åˆ¶ä¿å­˜çš„æ¶ˆæ¯æ•°é‡")
        
        # æ›´æ–°å…¨å±€å˜é‡
        MAX_MESSAGE_HISTORY = max_history
    
    # ç»Ÿè®¡ä¿¡æ¯
    st.divider()
    st.caption("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
    st.caption(f"æ€»è¯·æ±‚æ•°: {st.session_state.request_count}")
    st.caption(f"ç¼“å­˜æ¡ç›®: {len(st.session_state.cache)}")
    st.caption(f"å½“å‰æ¶ˆæ¯æ•°: {len(st.session_state.messages)}")
    
    # æ¸…é™¤æŒ‰é’®
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯å†å²"):
            st.session_state.messages = []
            st.rerun()
    
    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤ç¼“å­˜"):
            st.session_state.cache = {}
            st.success("ç¼“å­˜å·²æ¸…é™¤")

# ============================================================================
# ä¸»ç•Œé¢ï¼šæ˜¾ç¤ºå†å²æ¶ˆæ¯
# ============================================================================

st.title("ğŸ¤– Chatbot (ä¼˜åŒ–ç‰ˆ)")

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ============================================================================
# ä¸»ç•Œé¢ï¼šå¤„ç†ç”¨æˆ·è¾“å…¥
# ============================================================================

if prompt := st.chat_input("è¾“å…¥ä½ çš„æ¶ˆæ¯..."):
    # è¾“å…¥éªŒè¯
    if len(prompt) > 1000:
        st.error("âŒ è¾“å…¥è¿‡é•¿ï¼Œè¯·é™åˆ¶åœ¨ 1000 å­—ç¬¦ä»¥å†…")
        st.stop()
    
    # é€Ÿç‡é™åˆ¶æ£€æŸ¥
    can_proceed, wait_time = check_rate_limit()
    if not can_proceed:
        st.warning(f"â³ è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç­‰å¾… {wait_time:.1f} ç§’åå†è¯•")
        st.stop()
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # é™åˆ¶æ¶ˆæ¯å†å²é•¿åº¦
    limit_message_history()
    
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ç”Ÿæˆå›å¤
    with st.chat_message("assistant"):
        # å…ˆæ£€æŸ¥ Token
        if not hf_token:
            st.error("âŒ **æœªè®¾ç½® Token**")
            st.warning("è¯·åœ¨ä¾§è¾¹æ è¾“å…¥ä½ çš„ Hugging Face Token")
            st.info("**å¦‚ä½•è·å– Tokenï¼š**")
            st.write("1. è®¿é—® https://huggingface.co/settings/tokens")
            st.write("2. ç‚¹å‡» 'New token' åˆ›å»ºæ–° Token")
            st.write("3. å¤åˆ¶ Tokenï¼ˆä»¥ `hf_` å¼€å¤´ï¼‰")
            st.write("4. ç²˜è´´åˆ°ä¾§è¾¹æ çš„ Token è¾“å…¥æ¡†")
            st.stop()
        
        # æ£€æŸ¥ Token æ ¼å¼
        if not hf_token.startswith("hf_"):
            st.error("âŒ **Token æ ¼å¼é”™è¯¯**")
            st.warning(f"Token åº”è¯¥ä»¥ `hf_` å¼€å¤´")
            st.info(f"å½“å‰è¾“å…¥ï¼š`{hf_token[:20]}...`ï¼ˆå·²éšè—ï¼‰")
            st.info("è¯·æ£€æŸ¥ Token æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å‰å¾€ https://huggingface.co/settings/tokens é‡æ–°ç”Ÿæˆ")
            st.stop()
        
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                # è·å–å®¢æˆ·ç«¯ï¼ˆä¼˜åŒ–1: å®¢æˆ·ç«¯å¤ç”¨ï¼‰
                client = get_client(hf_token)
                
                # æ„å»ºæ¶ˆæ¯å†å²
                messages_for_api = [
                    {"role": msg["role"], "content": msg["content"]}
                    for msg in st.session_state.messages
                ]
                
                # è·å–æœ€è¿‘çš„æ¶ˆæ¯ï¼ˆç”¨äºç¼“å­˜é”®ï¼‰
                recent_messages = messages_for_api[-5:] if len(messages_for_api) > 5 else messages_for_api
                
                # æ£€æŸ¥ç¼“å­˜ï¼ˆä¼˜åŒ–2: æ·»åŠ ç¼“å­˜ï¼‰
                cache_key = get_cache_key(prompt, model_name, recent_messages) if use_cache else None
                cached_reply = get_cached_reply(cache_key) if cache_key else None
                
                if cached_reply:
                    # ä½¿ç”¨ç¼“å­˜çš„å›å¤
                    reply = cached_reply
                    st.info("ğŸ’¡ ä½¿ç”¨ç¼“å­˜ç»“æœ")
                else:
                    # è°ƒç”¨ API
                    # æ„å»ºæç¤ºè¯ï¼ˆç”¨äºæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼‰
                    # å¯¹è¯æ¨¡å‹åˆ—è¡¨
                    chat_models_list = [
                        "moonshotai/Kimi-K2-Thinking",
                        "deepseek-ai/DeepSeek-V3.2",
                        "zai-org/GLM-4.7-Flash",
                        "zai-org/GLM-4.7",
                        "meta-llama/Llama-3.1-8B-Instruct",
                        "openai/gpt-oss-20b",
                        "openai/gpt-oss-120b",
                        "MiniMaxAI/MiniMax-M2.1"
                    ]
                    
                    if model_name not in chat_models_list:
                        prompt_text = "\n".join([
                            f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
                            for msg in messages_for_api[-5:]
                        ])
                        prompt_text += "\nAssistant:"
                    else:
                        prompt_text = None
                    
                    # é€‰æ‹©è°ƒç”¨æ–¹å¼
                    # é»˜è®¤ä½¿ç”¨ InferenceClientï¼ˆå·²æ›´æ–°æ”¯æŒæ–°ç«¯ç‚¹ï¼‰ï¼Œå¤‡ç”¨æ–¹æ³•ä½œä¸ºå¤‡é€‰
                    if use_direct_api:
                        # ä½¿ç”¨å¤‡ç”¨æ–¹æ³•ï¼ˆç›´æ¥ requests è°ƒç”¨ï¼‰
                        reply = call_api_direct(hf_token, model_name, messages_for_api, prompt_text)
                    elif use_async:
                        # å¼‚æ­¥è°ƒç”¨
                        reply = call_api_async(client, model_name, messages_for_api, prompt_text)
                    else:
                        # åŒæ­¥è°ƒç”¨ï¼ˆåŸæœ‰æ–¹å¼ï¼‰
                        # å¯¹è¯æ¨¡å‹åˆ—è¡¨å·²åœ¨ä¸Šé¢å®šä¹‰ï¼Œè¿™é‡Œå¤ç”¨
                        if model_name in chat_models_list:
                            # å¯¹è¯æ¨¡å‹
                            response = client.chat_completion(
                                model=model_name,
                                messages=messages_for_api,
                                max_tokens=150
                            )
                            # å®‰å…¨åœ°è®¿é—®å“åº”å†…å®¹
                            if hasattr(response, 'choices') and len(response.choices) > 0:
                                if hasattr(response.choices[0], 'message'):
                                    reply = response.choices[0].message.content
                                else:
                                    reply = str(response.choices[0])
                            else:
                                reply = str(response)
                        else:
                            # æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
                            response = client.text_generation(
                                prompt_text,
                                model=model_name,
                                max_new_tokens=150,
                                temperature=0.7,
                                return_full_text=False  # åªè¿”å›æ–°ç”Ÿæˆçš„æ–‡æœ¬ï¼Œä¸åŒ…æ‹¬è¾“å…¥
                            )
                            # text_generation å¯èƒ½è¿”å›å­—ç¬¦ä¸²æˆ–ç”Ÿæˆå™¨
                            if isinstance(response, str):
                                reply = response
                            elif hasattr(response, '__iter__'):
                                # å¦‚æœæ˜¯ç”Ÿæˆå™¨æˆ–è¿­ä»£å™¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                                try:
                                    reply = ''.join(response) if not isinstance(response, str) else response
                                except StopIteration:
                                    # å¤„ç† StopIterationï¼ˆè¿­ä»£å™¨è€—å°½ï¼‰
                                    reply = str(response) if response else "ç”Ÿæˆå¤±è´¥ï¼šå“åº”ä¸ºç©º"
                            else:
                                reply = str(response)
                    
                    # ä¿å­˜åˆ°ç¼“å­˜ï¼ˆä¼˜åŒ–2: æ·»åŠ ç¼“å­˜ï¼‰
                    if cache_key and use_cache:
                        save_to_cache(cache_key, reply)
                    
                    # æ›´æ–°è¯·æ±‚è®¡æ•°
                    st.session_state.request_count += 1
                
                # æ˜¾ç¤ºå›å¤
                st.markdown(reply)
                
                # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
                st.session_state.messages.append({"role": "assistant", "content": reply})
                
                # å†æ¬¡é™åˆ¶æ¶ˆæ¯å†å²ï¼ˆæ·»åŠ æ–°æ¶ˆæ¯åï¼‰
                limit_message_history()
                
            except StopIteration as e:
                # ä¸“é—¨å¤„ç† StopIteration é”™è¯¯ï¼Œè‡ªåŠ¨å°è¯•å¤‡ç”¨æ–¹æ³•
                st.warning("âš ï¸ **æ£€æµ‹åˆ°å“åº”è§£æé”™è¯¯ï¼Œæ­£åœ¨å°è¯•å¤‡ç”¨æ–¹æ³•...**")
                
                try:
                    # è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨ API æ–¹æ³•é‡è¯•
                    # å¯¹è¯æ¨¡å‹åˆ—è¡¨
                    chat_models_list_retry = [
                        "moonshotai/Kimi-K2-Thinking",
                        "deepseek-ai/DeepSeek-V3.2",
                        "meta-llama/Llama-3.1-8B-Instruct"
                    ]
                    if model_name not in chat_models_list_retry:
                        prompt_text_retry = "\n".join([
                            f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
                            for msg in messages_for_api[-5:]
                        ])
                        prompt_text_retry += "\nAssistant:"
                    else:
                        prompt_text_retry = None
                    
                    reply = call_api_direct(hf_token, model_name, messages_for_api, prompt_text_retry)
                    st.success("âœ… **å·²ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè·å–å›å¤**")
                    
                    # æ˜¾ç¤ºå›å¤
                    st.markdown(reply)
                    
                    # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
                    st.session_state.messages.append({"role": "assistant", "content": reply})
                    
                    # é™åˆ¶æ¶ˆæ¯å†å²
                    limit_message_history()
                    
                    # æ›´æ–°è¯·æ±‚è®¡æ•°
                    st.session_state.request_count += 1
                    
                except Exception as retry_error:
                    # å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥äº†
                    st.error("ğŸ”„ **å“åº”è§£æé”™è¯¯ (StopIteration)**")
                    st.warning("**é—®é¢˜ï¼š** API è¿”å›çš„å“åº”æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
                    st.info("**å¯èƒ½çš„åŸå› ï¼š**")
                    st.write("1. Hugging Face API å“åº”æ ¼å¼å˜åŒ–")
                    st.write("2. æ¨¡å‹è¿”å›äº†ç‰¹æ®Šæ ¼å¼çš„å“åº”")
                    st.write("3. API ç‰ˆæœ¬ä¸å…¼å®¹")
                    st.write("4. æ¨¡å‹å¯èƒ½éœ€è¦ç‰¹æ®Šæƒé™æˆ–é…ç½®")
                    st.info("**è§£å†³æ–¹æ¡ˆï¼š**")
                    st.write("1. âœ… å·²è‡ªåŠ¨å°è¯•å¤‡ç”¨æ–¹æ³•ï¼ˆå¤±è´¥ï¼‰")
                    st.write("2. å°è¯•åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å‹ï¼ˆå¦‚ `moonshotai/Kimi-K2-Thinking` æˆ– `deepseek-ai/DeepSeek-V3.2`ï¼‰")
                    st.write("3. æ£€æŸ¥ Hugging Face API æ–‡æ¡£æ˜¯å¦æœ‰æ›´æ–°")
                    st.write("4. æ›´æ–° `huggingface_hub` åº“ï¼š`pip install --upgrade huggingface_hub`")
                    st.write("5. æŸäº›æ¨¡å‹ï¼ˆå¦‚ Llamaï¼‰å¯èƒ½éœ€è¦ç”³è¯·è®¿é—®æƒé™")
                    
                    with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯"):
                        st.code(f"åŸå§‹é”™è¯¯ç±»å‹: StopIteration\nåŸå§‹é”™è¯¯ä¿¡æ¯: {str(e)}\né‡è¯•é”™è¯¯ç±»å‹: {type(retry_error).__name__}\né‡è¯•é”™è¯¯ä¿¡æ¯: {str(retry_error)}\næ¨¡å‹: {model_name}\nToken å·²è®¾ç½®: {'æ˜¯' if hf_token else 'å¦'}")
                    
                    # è®°å½•é”™è¯¯æ—¥å¿—
                    if "error_log" not in st.session_state:
                        st.session_state.error_log = []
                    st.session_state.error_log.append({
                        "time": time.time(),
                        "error_type": "StopIteration",
                        "error": str(e),
                        "retry_error": str(retry_error),
                        "model": model_name
                    })
                
            except Exception as e:
                # è¯¦ç»†çš„é”™è¯¯å¤„ç†å’Œè¯Šæ–­
                error_type = type(e).__name__
                error_str = str(e)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ HTTP é”™è¯¯
                if "HTTPError" in error_type or "401" in error_str or "Unauthorized" in error_str:
                    st.error("ğŸ” **è®¤è¯é”™è¯¯**")
                    st.warning("**å¯èƒ½çš„åŸå› ï¼š**")
                    st.write("1. âŒ Token æœªè¾“å…¥æˆ–ä¸ºç©º")
                    st.write("2. âŒ Token æ ¼å¼é”™è¯¯ï¼ˆåº”è¯¥ä»¥ `hf_` å¼€å¤´ï¼‰")
                    st.write("3. âŒ Token å·²è¿‡æœŸæˆ–æ— æ•ˆ")
                    st.write("4. âŒ Token æ²¡æœ‰è®¿é—®è¯¥æ¨¡å‹çš„æƒé™")
                    
                    # æ£€æŸ¥ Token çŠ¶æ€
                    if not hf_token:
                        st.error("âš ï¸ **å½“å‰çŠ¶æ€ï¼šæœªæ£€æµ‹åˆ° Token**")
                        st.info("è¯·åœ¨ä¾§è¾¹æ è¾“å…¥ä½ çš„ Hugging Face Token")
                    else:
                        # æ£€æŸ¥ Token æ ¼å¼
                        if not hf_token.startswith("hf_"):
                            st.error(f"âš ï¸ **Token æ ¼å¼é”™è¯¯**")
                            st.info(f"Token åº”è¯¥ä»¥ `hf_` å¼€å¤´ï¼Œå½“å‰ï¼š`{hf_token[:10]}...`")
                        else:
                            st.error(f"âš ï¸ **Token å¯èƒ½æ— æ•ˆ**")
                            st.info("è¯·æ£€æŸ¥ Token æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å‰å¾€ https://huggingface.co/settings/tokens é‡æ–°ç”Ÿæˆ")
                
                elif "404" in error_str or "Not Found" in error_str:
                    st.error("ğŸ” **æ¨¡å‹æœªæ‰¾åˆ°**")
                    st.warning(f"**é—®é¢˜ï¼š** æ¨¡å‹ `{model_name}` ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®")
                    st.info("**è§£å†³æ–¹æ¡ˆï¼š**")
                    st.write("1. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
                    st.write("2. å°è¯•åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å‹ï¼ˆå¦‚ `moonshotai/Kimi-K2-Thinking`ï¼‰")
                    st.write("3. æŸäº›æ¨¡å‹å¯èƒ½éœ€è¦ç‰¹å®šçš„ Token æƒé™")
                
                elif "503" in error_str or "loading" in error_str.lower():
                    st.warning("â³ **æ¨¡å‹æ­£åœ¨åŠ è½½**")
                    st.info("Hugging Face æœåŠ¡å™¨æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»åé‡è¯•")
                    st.info("ğŸ’¡ æç¤ºï¼šå…è´¹æ¨¡å‹é¦–æ¬¡è°ƒç”¨éœ€è¦åŠ è½½æ—¶é—´ï¼Œé€šå¸¸éœ€è¦ 10-30 ç§’")
                
                elif "timeout" in error_str.lower() or "Timeout" in error_type:
                    st.error("â±ï¸ **è¯·æ±‚è¶…æ—¶**")
                    st.warning("API è°ƒç”¨è¶…æ—¶ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–æœåŠ¡å™¨ç¹å¿™")
                    st.info("**è§£å†³æ–¹æ¡ˆï¼š**")
                    st.write("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
                    st.write("2. ç¨åé‡è¯•")
                    st.write("3. å°è¯•ä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼ˆä¾§è¾¹æ  â†’ é«˜çº§è®¾ç½®ï¼‰")
                
                elif "Connection" in error_type or "è¿æ¥" in error_str:
                    st.error("ğŸŒ **ç½‘ç»œè¿æ¥é”™è¯¯**")
                    st.warning("æ— æ³•è¿æ¥åˆ° Hugging Face API")
                    st.info("**è§£å†³æ–¹æ¡ˆï¼š**")
                    st.write("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
                    st.write("2. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®")
                    st.write("3. å¦‚æœåœ¨ä¸­å›½å¤§é™†ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨ä»£ç†")
                
                elif "410" in error_str or "no longer supported" in error_str.lower() or "router.huggingface.co" in error_str.lower():
                    st.error("ğŸ”„ **API ç«¯ç‚¹å·²æ›´æ–°**")
                    st.warning("**é—®é¢˜ï¼š** Hugging Face API ç«¯ç‚¹å·²æ›´æ”¹")
                    st.info("**è§£å†³æ–¹æ¡ˆï¼š**")
                    st.write("1. âœ… **å·²æ›´æ–°ä»£ç ä½¿ç”¨æ–°ç«¯ç‚¹**")
                    st.write("2. ç¡®ä¿ `huggingface_hub` åº“æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼š")
                    st.code("pip install --upgrade huggingface_hub", language="bash")
                    st.write("3. åˆ·æ–°é¡µé¢ï¼Œä»£ç å·²é»˜è®¤ä½¿ç”¨ InferenceClientï¼ˆæ”¯æŒæ–°ç«¯ç‚¹ï¼‰")
                    st.write("4. å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œå°è¯•å–æ¶ˆå‹¾é€‰ 'ä½¿ç”¨å¤‡ç”¨ API æ–¹æ³•'")
                    
                    with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯"):
                        st.code(f"é”™è¯¯ç±»å‹: {error_type}\né”™è¯¯ä¿¡æ¯: {error_str}\næ¨¡å‹: {model_name}\nToken å·²è®¾ç½®: {'æ˜¯' if hf_token else 'å¦'}")
                        st.info("ğŸ’¡ InferenceClient åº”è¯¥å·²ç»æ”¯æŒæ–°ç«¯ç‚¹ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•")
                
                else:
                    # å…¶ä»–æœªçŸ¥é”™è¯¯
                    st.error(f"âŒ **é”™è¯¯ç±»å‹ï¼š** {error_type}")
                    st.error(f"**é”™è¯¯ä¿¡æ¯ï¼š** {error_str}")
                    st.info("ğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥ä½ çš„ Token æ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…å°è¯•æ›´æ¢æ¨¡å‹")
                
                # æ˜¾ç¤ºè¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼ˆå¯å±•å¼€ï¼‰
                with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯"):
                    st.code(f"é”™è¯¯ç±»å‹: {error_type}\né”™è¯¯ä¿¡æ¯: {error_str}\næ¨¡å‹: {model_name}\nToken å·²è®¾ç½®: {'æ˜¯' if hf_token else 'å¦'}")
                
                # è®°å½•é”™è¯¯æ—¥å¿—
                if "error_log" not in st.session_state:
                    st.session_state.error_log = []
                st.session_state.error_log.append({
                    "time": time.time(),
                    "error_type": error_type,
                    "error": error_str,
                    "model": model_name
                })

